# Multi-Model Advisor MCP Server Configuration
# This file demonstrates available environment variables.
# You can also override these using command-line arguments when starting the server.

# Server configuration
SERVER_NAME=multi-model-advisor
SERVER_VERSION=1.0.0
DEBUG=false

# Ollama configuration
# URL where Ollama API is running
OLLAMA_API_URL=http://localhost:11434

# Comma-separated list of models to query
# Example: gemma3:1b,llama3.2:1b,neural-chat
DEFAULT_MODELS=gemma3:1b,llama3.2:1b,deepseek-r1:1.5b

# Job Queue Configuration
MAX_CONCURRENT_JOBS=3           # Max concurrent jobs (1-100, default: 3)
RETRY_MAX_ATTEMPTS=4            # Max retry attempts (1-10, default: 4)
RETRY_INITIAL_DELAY_MS=1000     # Initial retry delay in ms (100-10000, default: 1000)
RETRY_MAX_DELAY_MS=8000         # Max retry delay in ms (1000-60000, default: 8000)

# System prompts for each model (use ANY of these methods):
# These define the "personality" or role of each model

# METHOD 1: Legacy model-specific (if using exact model names)
GEMMA_SYSTEM_PROMPT=You are a creative and innovative AI assistant. Think outside the box and offer novel perspectives.
LLAMA_SYSTEM_PROMPT=You are a supportive and empathetic AI assistant focused on human well-being. Provide considerate and balanced advice.
DEEPSEEK_SYSTEM_PROMPT=You are a logical and analytical AI assistant. Think step-by-step and explain your reasoning clearly.

# METHOD 2: Indexed by position (MODEL_1_PROMPT, MODEL_2_PROMPT, etc)
# Use this for any models in any order
# MODEL_1_PROMPT=First model's system prompt
# MODEL_2_PROMPT=Second model's system prompt
# MODEL_3_PROMPT=Third model's system prompt

# ============================================================================
# COMMAND-LINE USAGE (overrides environment variables):
# ============================================================================
# node build/index.js [OPTIONS]
#
# Options:
#   --server-name NAME              Server name (default: multi-model-advisor)
#   --server-version VERSION        Server version (default: 1.0.0)
#   --debug                         Enable debug mode (flag, no value needed)
#   --ollama-url URL                Ollama API URL (default: http://localhost:11434)
#   --models MODEL1,MODEL2,...      Comma-separated list of models
#   --model1-prompt "PROMPT"        System prompt for 1st model (works with ANY models!)
#   --model2-prompt "PROMPT"        System prompt for 2nd model
#   --model3-prompt "PROMPT"        System prompt for 3rd model
#   --max-concurrent-jobs NUM       Max concurrent jobs (1-100, default: 3)
#   --retry-attempts NUM            Max retry attempts (1-10, default: 4)
#   --retry-initial-delay NUM       Initial retry delay in ms (default: 1000)
#   --retry-max-delay NUM           Max retry delay in ms (default: 8000)
#   --gemma-prompt "PROMPT TEXT"    Legacy: Custom prompt for Gemma model
#   --llama-prompt "PROMPT TEXT"    Legacy: Custom prompt for Llama model
#   --deepseek-prompt "PROMPT TEXT" Legacy: Custom prompt for Deepseek model
#
# EXAMPLES:
# ============================================================================
#
# 1. Start with defaults (from .env):
#    node build/index.js
#
# 2. Override Ollama URL and enable debug:
#    node build/index.js --ollama-url http://192.168.1.100:11434 --debug
#
# 3. Use different models:
#    node build/index.js --models llama3:latest,neural-chat,mistral
#
# 4. Dynamic prompts for ANY models (recommended):
#    node build/index.js \
#      --models llama3:latest,neural-chat,mistral \
#      --model1-prompt "You are funny" \
#      --model2-prompt "You are helpful" \
#      --model3-prompt "You are analytical"
#
# 5. With concurrency and retry configuration:
#    node build/index.js \
#      --max-concurrent-jobs 5 \
#      --retry-attempts 5 \
#      --retry-initial-delay 500
#
# 6. Legacy model-specific prompts (for gemma, llama, deepseek):
#    node build/index.js \
#      --models gemma3:1b,llama3.2:1b \
#      --gemma-prompt "You are creative" \
#      --llama-prompt "You are empathetic"
#
# 7. Mix CLI args with environment variables (CLI takes precedence):
#    OLLAMA_API_URL=http://remote:11434 node build/index.js --debug --models llama3:latest
#
# ============================================================================
